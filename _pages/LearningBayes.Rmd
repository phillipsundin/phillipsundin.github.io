---
title: "LearningBayes"
author: "Sundin"
date: "6/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Bayesian Intro

Let me begin by making something very clear - I am not a Bayesian statistician. That is not to say that I do not like Bayesian statistics, nor I am fundamentally against Bayesian statistics. Rather, I do not know much about this vast branch of the statistics literature, and this is my first attempt to get a grasp on Bayesian statistics. 

Let's start with a simple linear regression. Under that framework, we assume that the distribution of outcome vector $\boldsymbol{Y}$ is normal such that $p(\boldsymbol{Y}|X,\beta) \sim N(X\beta, \sigma^2)$. It can be shown that a normal prior of the regression coefficients, $\beta$ is a conjugate distribution for linear regression. So if we assume that $Y$ is normally distributed and we have a normal prior for $p(\beta)$, then the poster $p(\beta | y,X,\sigma^2)$ will also be normally distributed. Although I've omitted the algebra, it can be shown that the posterior for $\beta$ is normal. Define 


```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

data ( chapt e r9 ) ; y<−yX. o2uptake [ , 1 ] ; X<−yX. o2uptake [ , −1]
g<−l eng th ( y ) ; nu0<−1 ; s20 <−8.54
S<−1000
## data : y , X
## p r i o r parameter s : g , nu0 , s20
## number o f independent samples to g ene r a t e : S
n<−dim(X) [ 1 ] ; p<−dim(X) [ 2 ]
Hg<− ( g /( g+1))  X%%s o l v e ( t (X)%%X)%%t (X)
SSRg<− t ( y)%%( diag ( 1 , nrow=n) − Hg ) %%y
s2<−1/rgamma(S , ( nu0+n )/2 , ( nu0 s20+SSRg)/2 )
Vb<− g s o l v e ( t (X)%%X) / ( g+1)
Eb<− Vb%%t (X)%%y
E<−matrix ( rnorm(Sp , 0 , s q r t ( s2 ) ) , S , p)
beta<−t ( t (E%%chol (Vb) ) +c (Eb) )